---
title: "HW3 - Getting Classy"
author: "Applied Machine Learning in Economics"
date: ""
output: 
  html_document:
    toc: yes
    number_sections: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')#, fig.width = 4, fig.height = 2.25)
```

Please submit as a print-to-pdf of the knitted html document as `HW3 firstname lastname.pdf`.
Put you answers inside appropriate section headers in your document.
Ensure your first and last name is in the document title area.

***********************************

# [20 points] Theory
***********************
When I showed how we can rearrange the sigmoid function to probabilities in the Regression-based Classification lecture, I was lazy and skipped several steps.
Help me out by showing me how to show my work.

Show how we can start with $p(X) = \frac{e^{\alpha + X \beta}}{1+e^{\alpha + X \beta}}$ and rearrange to get $log \left( \frac{p(X)}{1-p(X)} \right) = \alpha + X \beta$ in **at least five lines** (there should be at least five `=` signs)

As per usual, insert a picture of a handwritten copy.

$$
\begin{align*}
  p(X) & = \frac{e^{\alpha + X \beta}}{1+e^{\alpha + X \beta}}\\
  & \vdots\\
  log \left( \frac{p(X)}{1-p(X)} \right) & = \alpha + X \beta
\end{align*}
$$


**20/20 points - $\geq$ 5 lines**

**10/20 points - 4 lines**

**0/20 points - $\leq$ 3 lines**

*Here a line corresponds to how many equal signs. For example, I have shown two above*

# Application
**********************************

For all models, blindly use the right hand side (RHS) below : (this homework isn't about model selection. We should do better than this for real applications):

`~ degree + poly(age,2) + female*anykids + hispan + race + renter + health + rgdp_growth`
`+ log(rgdpc) + hpi + coll_share + urate + log(occwage) + log(ftotval)`

## [10 points] Setup
****************************

In one code chunk, read in the homework data as `cps` as a tibble and load the following packages:

- `tidyverse`
- `data.table` (optional)
- `nnet`
- `class`
- `MASS`
- `plotROC`
- `caret`

You may do so in any order.

```{r, include = FALSE, eval = FALSE, message = FALSE}
library(tidyverse)
library(nnet)
library(class)
library(MASS)
library(plotROC)
library(caret)

cps = read.csv("C:/Users/johnj/Documents/Data/Applied ML ECON490/hw data/hw data.csv") %>% as_tibble
```
**[7 POINTS]**

- **[1 POINT] for each package (excluding `data.table`) and loading in data as a tibble**


Run this code:
```{r, eval = FALSE}
cps$id = row.names(cps)
```

Split the data into a train and test set, where the training data is 67% of the size of cps using `sample_frac` and `anti_join` by `id`.
Call the training data `train`, the testing data `test`, and set the seed to `490`.

- **[1 POINT] for appropriate 67% split** 
- **[1 POINT] for correct training data**
- **[1 POINT] for correct testing data**
- **[-10 POINTS] for not setting the seed to 490**


```{r, include = FALSE, eval = FALSE}
set.seed(490)
train = cps %>% sample_frac(0.67)
test  = anti_join(cps, train, by = 'id')
```


## Labor Force Participation
**************************************


### [5 points] Plots
***********************************
Using your favorite variable from the list below, replicate the LPM plot and logistic plot as outlined in `Lecture 7 Figures.R` on `cps`. 
Use `lf` as the y variable.

Variables:

- `ftotval`
- `age`
- `health`
- `hpi`
- `coll_share`
- `urate`
- `occ_wage`

On both figures include:

- x and y axis labels
- A title
- Size 20 text
- Size 2 line
- A predicted line in your favorite color

**[MINUS 1 POINT] per missing element. DON'T GET A NEGATIVE SCORE!**

```{r, include = FALSE, eval = FALSE}

ggplot(cps, aes(x = health, y = lf)) + 
  stat_bin_2d(bins = 4) +
  geom_smooth(se = FALSE, method = 'lm', color = '#8803fc', size = 2) +
  labs(x = 'Health', y = 'Labor Force Participant', title = 'Labor Force Status vs. Health') +
  theme(text = element_text(size = 20))

ggplot(cps, aes(x = health, y = lf)) + 
  stat_bin_2d(bins = 4) +
  geom_smooth(se = FALSE, method = 'glm', method.args = list(family = 'binomial'), color = '#8803fc', size = 2) +
  labs(x = 'Health', y = 'Labor Force Participant', title = 'Labor Force Status vs. Health') +
  theme(text = element_text(size = 20))
```



### [4 points] LPM
*****************************************

Using the RHS from above, fit an LPM on `train` named `linfit`. Print the summary of `linfit`.

**[ALL POINTS] for correct summary**

```{r, include = FALSE, eval = FALSE}
linfit = lm(lf ~ degree + poly(age,2) + female*anykids + hispan + race + renter + health + rgdp_growth + log(rgdpc) + hpi + coll_share + urate + log(occwage) + log(ftotval), train)
summary(linfit)
```

Pay attention to the marginal effect of `log(occwage)` for later.

### [5 points] Binomial Logistic Regression
*************************************************

Using the same formula as the LPM model, fit a logistic regression on `train` and save the object as `logfit`. Print the summary of `logfit`.

**[2 POINTS] for correct summary**

```{r, include = FALSE, eval = FALSE}
logfit = glm(lf ~ degree + poly(age,2) + female*anykids + hispan + race + renter + health + rgdp_growth + log(rgdpc) + hpi + coll_share + urate + log(occwage) + log(ftotval), train, family = "binomial")

summary(logfit)
```

Using the average predicted values method, produce the marginal effects for all coefficients besides the intercept from the predicted values generated from fitting `logfit` on `train`.

**[3 POINTS] for correct marginal effects given the model used above. NO POINTS AWARDED FOR USING `margins()`**

```{r, include = FALSE, eval = FALSE}
yhat = logfit$fitted.values

mean(  exp(yhat)/ (1+exp(yhat))^2 )*coef(logfit)[-1]
```


**[0 POINTS] How does the marginal effect of `log(occwage)` compare?**

### [4 points] KNN
************************************************

Here you will fit a KNN model using the `train` data on the same `matrix.model` grabbed from `linfit`.
Use the `test` data appropriately.
Follow the ML Classification lecture.
Set `k = 16` (I checked the best fit ahead of time). 

*This will take a while to fit. Remember this when knitting your document. It recalculates this EVERY knit.*

**[ALL POINTS] for `table(knnfit)`**

```{r, include = FALSE, eval = FALSE}
x_train = model.matrix(linfit)
y_train = train$lf

f = linfit$terms
x_test  = model.matrix(f, data = test)

knnfit = knn(x_train, x_test, y_train, 16)

table(knnfit)
```

### [4 points] LDA
*****************************************

Fit an LDA model on `train` using the same formula. Save the fitted mode as `ldafit`.

**[ALL POINTS] correct output of `ldafit$means`**

```{r, include = FALSE, eval = FALSE}
ldafit = lda(f, train)
ldafit$means
```

### [4 points] QDA

Fit a QDA model on `train` using the same formula. Save the fitted model as `ldafit`.

**[ALL POINTS] correct output of `qdafit$means`**

```{r, include = FALSE, eval = FALSE}
qdafit = qda(f, train)
qdafit$means
```



### [10 points] ROC
************************************

Fill in the following code:

```{r, eval = FALSE}
dflin = tibble(yhat = predict(linfit, test), y = test$lf, model = 'LPM')

dflog = tibble(yhat = predict( , , type = 'response'), y = test$lf, model = 'Logistic')

ldahat = 
dflda = tibble(yhat = ldahat$posterior[,2], y = test$lf, model = 'LDA')

qdahat = 
dfqda = tibble()

df = bind_rows(, , , )
```

**[2 POINTS] for correct `summary(df)`**

```{r, include = FALSE, eval = FALSE}
dflin = tibble(yhat = predict(linfit, test), y = test$lf, model = 'LPM')

dflog = tibble(yhat = predict(logfit, test, type = 'response'), y = test$lf, model = 'Logistic')

ldahat = predict(ldafit, test)
dflda = tibble(yhat = ldahat$posterior[,2], y = test$lf, model = 'LDA')

qdahat = predict(qdafit, test)
dfqda = tibble(yhat = qdahat$posterior[,2], y = test$lf, model = 'QDA')

df = bind_rows(dflin, dflog, dflda, dfqda)

summary(df)
```

**[4 POINTS] Produce an ROC curve for `df` called `final`.** Labels are not necessary.

```{r, include = FALSE, eval = FALSE}
(final = ggplot(df, aes(m = yhat, d = y, color = model)) +
  geom_roc(n.cuts = 10, labelround = 2) )
```

**[2 POINTS] output `calc_auc(final)`**
```{r, include = FALSE, eval = FALSE}
calc_auc(final)
```

Compare the accuracy of the best AUC model with using a threshold of 0.60 to the KNN model on `test`.
Use an `ifelse` statement like Regression-base Classification section 2.3.

**[2 POINTS] correctly chosen model given your output**

```{r, include = FALSE, eval = FALSE}
yhat = ifelse(predict(logfit, test, type = 'response') >= 0.6,
              1,
              0) 
mean(yhat == test$lf)

mean(knnfit == test$lf)

```
*The logistic model performs better*.


## Degree
***************************************************************************************

To wrap up, you are going to compare the performance of a multinomial logistic regression, KNN, LDA, and QDA on predicting what education bin an individual is in.

We are going to use a slightly modified `formula`. **Swap `degree` and `lf` in the model**.


### [5 points] Multinomial Logistic Regression
***************************************************
Fit a multinomial logistic regression on `train` called `logfit` using the RHS specified above. 
Create an object `loghat` of fitted values from `logfit` on `test`.


**[ALL POINTS] awarded for correct code.**

```{r, include = FALSE, eval = FALSE}
logfit = multinom(degree ~ lf + poly(age,2) + female*anykids + hispan + race + renter + health + rgdp_growth + log(rgdpc) + hpi + coll_share + urate + log(occwage) + log(ftotval), train)

loghat = predict(logfit, test)
```



### [5 points] KNN
***********************************************
Using the same formula as above, fit a KNN as `knnhat`. Set k = 16.
Don't forget to adjust the formula and create different `model.matrix` objects from the previous KNN.

Again, this will take a while to execute.

**[ALL POINTS] awarded for correct code.**

```{r, include = FALSE, eval = FALSE}
f = degree ~ lf + poly(age,2) + female*anykids + hispan + race + renter + health + rgdp_growth + log(rgdpc) + hpi + coll_share + urate + log(occwage) + log(ftotval)

x_train = model.matrix(logfit)
y_train = train$degree

x_test  = model.matrix(f, data = test)

knnhat = knn(x_train, x_test, y_train, 16)
```

### [5 points] LDA
**********************************************************
Fit an LDA on `train` called `ldafit` using the same formula.
Create an object `ldahat` of fitted values from `ldafit` on `test`.


**[ALL POINTS] awarded for correct code.**

```{r, include = FALSE, eval = FALSE}
ldafit = lda(f, train)
ldahat = predict(ldafit, test)
ldahat = ldahat$class
```


### [5 points] QDA
*********************************************************
Fit a QDA on `train` called `qdafit` using the same formula.
Create an object `qdahat` of fitted values from `qdafit` on `test`.


**[ALL POINTS] awarded for correct code.**

```{r, include = FALSE, eval = FALSE}
qdafit = qda(f, train)
qdahat = predict(qdafit, test)
qdahat = qdahat$class
```


### [10 points] Comparison
***********************************************************

Finally, calculate and show the error rate for each model. Which model would you choose based upon the output?

- **[5 POINTS] for correct code**
- **[5 POINTS] for choosing best model given the output of the code**

```{r, include = FALSE, eval = FALSE}
mean(loghat != test$degree)
mean(knnhat != test$degree)
mean(ldahat != test$degree)
mean(qdahat != test$degree)

# Based upon these error rates, I would choose the multinomial logistic regression
```












