---
title: "Support Vector Machines"
author: "Applied Machine Learning in Economics"
date: ""
output: 
  html_document:
    toc: yes
    number_sections: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', fig.width = 16, fig.height = 9)
```

Support vector machines have flexible decision boundaries that can lead to better classification models.
They can also be applied to regression problems.
This lecture will cover how to implement a multi-class classification problem and a regression problem. 
In both settings, we will compare the polynomial and radial kernel.


**WARNING** the `caret` package implementation of SVMs takes *forever* to run. 
Like goodness gracious, how is it coded so inefficiently?
Fortunately, the SVM package `kernlab` has built in cross-validation.
It still takes a while to run, but it is magnitudes (plural) faster than `caret`.

Only use SVM on smaller datasets, unless you literally have access to a supercomputer.

```{r libraries/seed , message = FALSE}
library(tidyverse)
library(kernlab)

# Setting the seed
set.seed(490)

# Starting the cloc
tic = proc.time()
```

In this script, we are going to show how to implement classification and regression SVM.
We are going to do a bad job at identifying hyperparameters.
Specifically, we are going to identify the hyperparameter for acceptable margin violations in the linear kernel setting and apply the next largest to the polynomial and radial kernels. 
There is no reason to believe that the different kernels should have the same hyperparameter.
We are using the next largest in hopes that the flexibility of the other models compensate for margin violations (*note that the larger the acceptable margin violations, the more computationally expensive it is to fit*).

Don't do this in practice. When actually working with SVMs, you should cross-validate all parameters for the different kernels independently.

# Classification
**********************************************************************

The first step is to load in the discrete lecture data and split it.
```{r discrete}
ahs = read.csv('C:/Users/johnj/Documents/Data/Applied ML ECON490/lecture data/lecture data discrete.csv') 

train = sample_frac(ahs, 0.67) 
test  = anti_join(ahs, train, by = 'id')

f = as.factor(renter) ~ log(cost) + hoa + log(hinc) + beds + baths  + lotsize + yos + condo + garage + bld + crime + hinf + houtf
```

## Linear Kernel
*****************************************
We will first start off by choosing three different values of $C$: 0, 1, and 10.
Whatever we choose, we will apply to the polynomial and radial kernels.
In practice you should cross-validate each model independently.
We will use 5-fold cross validation (about as small as you should go).

```{r class linear}
( fit_l_0 = ksvm(f, train, kernel = 'vanilladot', C = 0.1, cross = 5) )
( fit_l_1 = ksvm(f, train, kernel = 'vanilladot', C = 1, cross = 5) )
( fit_l_10 = ksvm(f, train, kernel = 'vanilladot', C = 10, cross = 5) )
```

It looks like the value of $C = 10$ performs the best on the training and cross-validation set, so we will erroneously apply $C=1$ to the subsequent kernels.

## Polynomial Kernel
********************************
Remember that any function is equivalent to an infinite order polynomial. 
I think you would have a hard time justifying fitting a model with higher than degree three, but we will only check  the first two degrees (why am I only using one model below if I want to check a first and second order polynomial?).


```{r class poly}
(  fit_p_2 = ksvm(f, train, kernel = 'polydot', C = 1, kpar = list(degree = 2), cross = 5)  )
```

It looks like the second order polynomial does better than the first order polynomial.

## Radial Kernel
************************
Finally, we will move to the radial kernel. 
We shall check the values of $\sigma \in \{0.1, 1}$.

```{r class radial}
(  fit_r_0 = ksvm(f, train, kernel = 'rbfdot', C = 1, kpar = list(sigma = 0.1), cross = 5)  )
(  fit_r_1 = ksvm(f, train, kernel = 'rbfdot', C = 1, kpar = list(sigma = 1), cross = 5)  )
```
Woah! It looks like $\sigma = 1$ overfits the training data!
However, $\sigma = 0.1$ appears to do a much better job indicated by the balance between the training and CV error rate.
We shall find out what model does the best job in next section!

## Comparison
**************************************
Should we refit these modes without cross-validation to maximize the signal in a real application?
You-betcha!
Are we going to in this lecture?
Nope!
*To fit without cross-validation, simply remove the `cross = 5` from `ksvm()`*.


```{r class comparison}
yhat_l = predict(fit_l_10, test)
yhat_p = predict(fit_p_2, test)
yhat_r = predict(fit_r_0, test)

y_test = test$renter %>% as.character # need to convert from integer

err_l = mean(yhat_l != y_test)
err_p = mean(yhat_p != y_test)
err_r = mean(yhat_r != y_test)

err_l; err_p; err_r

(tic - proc.time())/60 # Minutes
```

It appears that the radial kernel performs the best!

# Regression
***************************

One thing we like about SVM is their ability to fit a wide range of kernels to the data.
Cross-validation can tell us how flexible our models should be.

```{r continuous}
ahs = read.csv('C:/Users/johnj/Documents/Data/Applied ML ECON490/lecture data/lecture data continuous.csv') 

train = sample_frac(ahs, 0.67) 
test  = anti_join(ahs, train, by = 'id')

f = log(value) ~  log(cost) + hoa + log(hinc) + beds + baths  + lotsize + yos + condo + garage + bld + crime + hinf + houtf
```

We are going to proceed using the same methodology as above.

## Linear Kernel
**************************************

We are instead going to choose values of $C \in \{0.1, 1\}$ because we are reversing the objective of SVM.
We are also going to try values of $\epsilon \in \{0.1,1\}$ (insensitivity of the loss function).

```{r reg linear}
( fit_l_0_0 = ksvm(f, train, kernel = 'vanilladot', C = 0.1, epsilon = 0.1, cross = 5) )
( fit_l_0_1 = ksvm(f, train, kernel = 'vanilladot', C = 0.1, epsilon = 1, cross = 5) )
( fit_l_1_0 = ksvm(f, train, kernel = 'vanilladot', C = 1, epsilon = 0.1, cross = 5) )
( fit_l_1_1 = ksvm(f, train, kernel = 'vanilladot', C = 1, epsilon = 1, cross = 5) )
```

It looks like the model with $C = \epsilon = 1$ performs the best (marginally), so we will simply use those values for the subsequent kernels.

## Polynomial Kernel
*****************************

Again, let's only try the second order polynomial.

```{r reg poly}
( fit_p_2 = ksvm(f, train, kernel = 'polydot', C = 1, epsilon = 1, kpar = list(degree = 2), cross = 5) )
```


## Radial
***********************************


```{r reg radial}
( fit_r_0 = ksvm(f, train, kernel = 'rbfdot', C = 1, epsilon = 1, kpar = list(sigma = 0.1), cross = 5) )
( fit_r_1 = ksvm(f, train, kernel = 'rbfdot', C = 1, epsilon = 1, kpar = list(sigma = 1), cross = 5) )
```
Definitely the smaller $\sigma$.

## Comparison
********************************

Before we get to the actual numbers, I do want to point out that it appears we are overfitting regardless of the kernel.
How do we know that?

A possible solution would be to check for and remove irrelevant predictors.

For the MSEs below, we should refit the models. But we aren't. Lazy Julian...

```{r}
yhat_l = predict(fit_l_1_1, test)
yhat_p = predict(fit_p_2, test)
yhat_r = predict(fit_r_0, test)

y_test = log(test$value)

err_l = mean((yhat_l - y_test)^2)
err_p = mean((yhat_p - y_test)^2)
err_r = mean((yhat_r - y_test)^2)

err_l; err_p; err_r
(tic - proc.time())/60 # Minutes
```
It appears that the polynomial model performs better than the other two kernels. 


