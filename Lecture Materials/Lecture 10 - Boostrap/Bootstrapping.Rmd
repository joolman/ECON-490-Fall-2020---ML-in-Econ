---
title: "Bootstrapping"
author: "Applied Machine Learning in Economics"
date: ""
output: 
  html_document:
    toc: yes
    number_sections: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')#, fig.width = 4, fig.height = 2.25)
```


As you can tell from the video, this lecture will be shorter.
Because bootstrapping is a useful tool that is genius in its simplicity, we will manually do it.
Another look underneath the hood.
We will do the usual coefficient bootstrapping and a new technique: AB testing.

# Coefficient Bootstrapping
***************************************************************

Time for our preliminary setup:

```{r}
set.seed(490) # Why do we need to set the seed?
library(tidyverse)

ahs = read.csv('C:/Users/johnj/Documents/Data/Applied ML ECON490/Lecture Data/lecture data continuous.csv')
```


Suppose we are interested in the correlation between house values and the number of baths.
We want to make sure we include other variables to control for other characteristics of the house.
This produces the conditional correlation of the number of baths in a house.

We are also going to manually save our data.
There are two ways we can do this.
The first is to create a `NULL` object and bind coefficients to it.
The second is to generate an complete `matrix` or `data.frame` to overwrite elements.
The second is computationally more efficient due to the fact that it doesn't have to effectively create a new object every iteration. 
Let's do that.

```{r}
# Number of iterations
n = 500

# Creating our object to hold our results
vrbls = lm(log(value) ~ log(cost):hoa + npeople + log(hinc) + beds + baths + log(eqvval) + lotsize + yos + condo + garage + I(bld == 'Mobile Home') + poly(math, 2) - hoa, data = ahs) %>%
  coef %>%
  names

hold = matrix(NaN, ncol = length(vrbls), nrow = n)

index = 1:n
for(i in 1:n){
  hold[i, ] = lm(log(value) ~ log(cost):hoa + npeople + log(hinc) + beds + baths + log(eqvval) + lotsize + yos + condo + garage + I(bld == 'Mobile Home') + poly(math, 2) - hoa, data = ahs, subset = sample(index, n, replace = TRUE)) %>%
    coef 
}

```


So, lets create some summary statistics and a pretty plot.
```{r}
# Which column was the baths variable again?
grep('baths', vrbls)

(m = mean(hold[, 5]))
(s = sd(hold[, 5]))

df = data.frame(baths = hold[, 5])

ggplot(df, aes(x = baths)) +
  geom_histogram(fill = 'blue', color = 'white') +
  geom_vline(xintercept = m, size = 2, color = 'orange') +
  geom_vline(xintercept = c(m-s, m+s), size = 2, color = 'orange', linetype = 'dashed') +
  geom_vline(xintercept = c(m-2*s, m+2*s), size = 2, color = 'orange', linetype = 'dotted') +
  theme(text = element_text(size = 20)) +
  labs(x = 'Estimate', y = '', title = 'Bootstrapped Distribution of Baths: n = 500') +
  scale_y_continuous(breaks = NULL)
```
We have plotted some standard deviation lines, where the dashed is one standard deviation from the mean and the dotted is two standard deviations out.
Wait, what p-values do these correspond to again?


# AB Testing
**************************************************************


Have you ever gone to an optometrist where you put your eyes up to a phoropter, and they ask you “number 1 or number 2, number 1 or number 2, number 1 or number 3” etc.?
Well, that's kind of what AB testing is like.
This typically used in website development, where we want to see if one version of a website is more likely to have you click a link. 
They randomly show visitors website A or website B. 
Because we have random assignment, this allows us to obtain a causal effect. 


We will use some data from [kaggle](https://www.kaggle.com/samtyagi/audacity-ab-testing).
You may have heard of this company before...
Let's load in the data, do a small amount of EDA, and then perform some bootstrapping.

## EDA
**********************
```{r}
ab = read.csv('C:/Users/johnj/Documents/Data/Applied ML ECON490/lecture data/homepage_actions.csv')
head(ab)
table(ab$group)
table(ab$group, ab$action)
```
It looks like we have slightly more in the control group than the treatment group, but that is okay.
Just a simple table makes it look like the treatment group may have had relatively more clicks in percentage terms.

Before we get into the bootstrapping, let me show you how to make use of the date information.
```{r}
library(lubridate)

ab$datetime = ymd_hms(ab$timestamp)
summary(ab$datetime)
```
While we aren't going to use this information in this lecture, we *could* use it to give us more information.
Perhaps clicking habits differ by time of day (morning, lunchtime, evening, etc.), or day of the week, or time of the year (winter holidays, etc.).

To get the day of the week:
```{R}
weekdays(ab$datetime) %>% table
```

## Bootstrapping
****************
Without further ado, let's get into it.
```{r}
# Standard AB testing
prop.test(c(932, 928), c(4264, 3924))
# Where did I get these numbers from? Why did I put them in in these arguments?
```

We can also get a similar result using OLS. 
Remember, we are interested in the coefficient, not the predicted value.
Here we are running an LPM.
```{r}
df = data.frame(click = (ab$action == 'click'), treatment = (ab$group == 'experiment'))

lm(click ~ treatment, data = df) %>% summary
```
Now we can bootstrap
```{r}
iterations = 500
hold = matrix(NaN, nrow = iterations)

n = dim(df)[1]
index = 1:n

for(i in 1:iterations){
  hold[i] = coef(lm(click ~ treatment, df, subset = sample(index, n, replace = T)))[2]
}

mean(hold)
sd(hold)

hist(hold)

```
Just using a lazy `hist()` plot shows us that this coefficient is likely positive. 
Again, it is up to us to determine if this statistical significance is economically significant.
